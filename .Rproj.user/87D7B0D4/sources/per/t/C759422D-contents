#Import of CyAN data from NASA

#Scripts to download and process daily CyAN data (EPA) from NASA

# Using the NHD Layer for summarization instead of the EPA CyAN 'Resolvable' Layer

#Originally developed by Brian Fulfrost in 10/2020

#Modified by Dan Sobota in 5/2021

#Script functions

#This script downloads NASA OLCI imagery from NASA's website, calculates cyanobacteria abundance according to the US EPA CyAN project protocol,
#and calculates summary statistics for the 42 resolvable lakes/reservoirs in Oregon. See specific code chunks for the explanations of the code functions.
#This code is specific to Sentinel 3 data from 2016 to the present.

#Requirments:

# This script is run within the arcgispro-py3 virtual environment; you must have access to arcpy to run the scripts
# A Spatial Analyst license is needed to run the zonal statistics feature

# Import libraries

# from CyANconfig import * # Configuration file with file paths and website addresses; place in same folder as this python script
from NHDconfig import * # NHD layer information for non-resolvable CyAN waterbodies
import sys
import urllib.request
import shutil
import os
import datetime
import pandas as pd
import glob
from openpyxl import load_workbook
from simpledbf import Dbf5
import arcpy
from arcpy import env
from arcpy.sa import *
from datetime import date

# Check to make sure right version of python is in use (3.7.10)
print(sys.version_info)
if sys.version_info < (3, 7):
    print('Please upgrade your Python version to 3.7.0 or higher')
    sys.exit()
    
mosaicdir = os.path.join(extract_path, 'mosaic')

# Need to get a list of file names for zonal statistics
# Need to modify to account for existing files in the directory

mosaicfilename2 = list()
for i, file in enumerate(os.listdir(mosaicdir)):
    if file.endswith(".tif"):
        mosaicfilename2.append(os.path.basename(file))
        
mosaicfilename2 = mosaicfilename2[(hab_day_start - 1):(hab_day_end + 1)]

# Need to get a list of names for mosaic key
# Need to modify to account for existing files in the directory

mosaickey2 = list()
for i, file in enumerate(os.listdir(mosaicdir)):
    if file.endswith(".tif"):
        mosaickey2.append(os.path.splitext(file)[0])

mosaickey2 = mosaickey2[(hab_day_start - 1):(hab_day_end + 1)]

# Zonal stats
for i in range(0, hab_days_length):
    
    zonalraster = os.path.join(mosaicdir, mosaicfilename2[i])
    stats_dir = os.path.join(extract_path, 'mosaic', 'stats')

    thestatsname = os.path.join(os.path.join(stats_dir, mosaickey2[i] + "_stats.dbf"))
    print(thestatsname)

    # Process: Zonal Statistics as Table
    env.workspace = mosaicdir
    arcpy.gp.ZonalStatisticsAsTable_sa(zones, "GNISIDName", zonalraster, thestatsname, "DATA", "ALL")

    arcpy.DeleteField_management(thestatsname,["ZONE_CODE", "SUM", "MEDIAN", "PCT90"])

    #calcdaydate
    arcpy.AddField_management(thestatsname, 'Day', 'LONG')
    arcpy.AddField_management(thestatsname, 'Year', 'LONG')
    arcpy.AddField_management(thestatsname, 'Date', 'Date')

    # extract theyear and theday
    yeardate = mosaickey2[i]
    theyear = int(yeardate[0:4])
    theday = int(yeardate[4:7])

    # calc day and year
    arcpy.CalculateField_management(thestatsname, 'Day', theday, 'PYTHON')
    arcpy.CalculateField_management(thestatsname, 'Year', theyear, 'PYTHON')

    # calc date
    thedate = datetime.datetime(theyear, 1, 1) + datetime.timedelta(theday - 1)
    epoch = datetime.datetime(1899, 12, 30)
    days = (thedate - epoch).days
    arcpy.CalculateField_management(thestatsname, 'Date', days, 'PYTHON')

    # deleted temp rasters above and directory removal below now works
    shutil.rmtree(os.path.join(temp_dir[i], 'cellsml'), ignore_errors=True)
    shutil.rmtree(temp_dir[i], ignore_errors=True)
    shutil.rmtree(archive_dir[i], ignore_errors=True)

# Need to get a list of stat files for mosaic key

stats_dir = os.path.join(extract_path, 'mosaic', 'stats')

thestatsname = list()
for i, file in enumerate(os.listdir(stats_dir)):
    if file.endswith(".dbf"):
        thestatsname.append(os.path.basename(file))
        
thestatsname = thestatsname[(hab_day_start - 1):(hab_day_end + 1)]

# Convert table to df, rename fields, add new field, and append excel file
for i in range(0, hab_days_length):
    
    dbf = Dbf5(os.path.join(stats_dir, thestatsname[i]))
    df = dbf.to_dataframe()

    # add new field and rename existing fields
    df.insert(3, "PercentArea_Value", '', True)
    df.rename(columns={"MIN": "MIN_cellsml", "MAX": "MAX_cellsml", "RANGE": "RANGE_cellsml", "MEAN": "MEAN_cellsml", "STD": "STD_cellsml"})

    # append new data to exsiting excel spreadhseet
    thetable = os.path.join(dir_Shiny, 'HAB_resolvablelakes_2021.xlsx')
    writer = pd.ExcelWriter(thetable, engine='openpyxl', mode ='a')
    writer.book = load_workbook(thetable)
    writer.sheets = dict((ws.title, ws) for ws in writer.book.worksheets)
    # BKF - need to make sure that startrow isn't overwriting last row
    firstrow = writer.book['HAB_resolvable_lake_data'].max_row

    df.to_excel(writer, sheet_name='HAB_resolvable_lake_data', startrow=firstrow, startcol=0, index=False, header=None)

    writer.save()
    #writer.close

# Set workspace and directory for reprojection; manually adjust for now
env.workspace = os.path.join(extract_path, "mosaic")
arcpy.env.compression = "LZW"
arcpy.env.overwriteOutput = True
final_dir = os.path.join(extract_path, "mosaic", "web")
final_dir_Shiny = os.path.join(dir_Shiny, year)

# Set up spatial data for reprojection

arcpy.CheckOutExtension("Spatial")

for raster in mosaicfilename2:

    fileName, fileExtension = os.path.splitext(raster)
    tile = fileName[-8:]
    tmpfile = "temp.tif"
    newfile = (tile + fileExtension)
    print(os.path.join(final_dir_Shiny, tile + fileExtension))

    arcpy.env.extent = theextent
    arcpy.ProjectRaster_management(in_raster=raster, out_raster=os.path.join(final_dir_Shiny, tmpfile), out_coor_system="PROJCS['WGS_1984_Web_Mercator_Auxiliary_Sphere',GEOGCS['GCS_WGS_1984',DATUM['D_WGS_1984',SPHEROID['WGS_1984',6378137.0,298.257223563]],PRIMEM['Greenwich',0.0],UNIT['Degree',0.0174532925199433]],PROJECTION['Mercator_Auxiliary_Sphere'],PARAMETER['False_Easting',0.0],PARAMETER['False_Northing',0.0],PARAMETER['Central_Meridian',0.0],PARAMETER['Standard_Parallel_1',0.0],PARAMETER['Auxiliary_Sphere_Type',0.0],UNIT['Meter',1.0]]", resampling_type="NEAREST", cell_size="300 300", geographic_transform="WGS_1984_(ITRF00)_To_NAD_1983", Registration_Point="", in_coor_system="PROJCS['USA_Contiguous_Albers_Equal_Area_Conic_USGS_version',GEOGCS['GCS_North_American_1983',DATUM['D_North_American_1983',SPHEROID['GRS_1980',6378137.0,298.257222101]],PRIMEM['Greenwich',0.0],UNIT['Degree',0.0174532925199433]],PROJECTION['Albers'],PARAMETER['False_Easting',0.0],PARAMETER['False_Northing',0.0],PARAMETER['Central_Meridian',-96.0],PARAMETER['Standard_Parallel_1',29.5],PARAMETER['Standard_Parallel_2',45.5],PARAMETER['Latitude_Of_Origin',23.0],UNIT['Meter',1.0]]")
    tmpraster = os.path.join(final_dir_Shiny, tmpfile)
    finalraster = os.path.join(final_dir_Shiny, newfile)

    arcpy.Clip_management(in_raster=tmpraster, rectangle="-13975366.498500 5052033.819600 -12850574.046900 5935465.862100",
                          out_raster=finalraster,
                          in_template_dataset=themask, nodata_value="-3.402823e+038",
                          clipping_geometry="ClippingGeometry", maintain_clipping_extent="NO_MAINTAIN_EXTENT")
